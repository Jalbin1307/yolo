{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a3ba445",
   "metadata": {},
   "source": [
    "# Download VOC Dataset\n",
    "link : https://pjreddie.com/projects/pascal-voc-dataset-mirror/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "950780b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wget\n",
    "import os\n",
    "\n",
    "%mkdir train\n",
    "%mkdir test\n",
    "\n",
    "train_url = 'http://pjreddie.com/media/files/VOCtrainval_06-Nov-2007.tar'\n",
    "test_url = 'http://pjreddie.com/media/files/VOCtest_06-Nov-2007.tar'\n",
    "wget.download(train_url,'train/')\n",
    "wget.download(test_url,'test/')\n",
    "\n",
    "!tar -xf test/VOCtest_06-Nov-2007.tar -C test/\n",
    "!tar -xf train/VOCtrainval_06-Nov-2007.tar -C train/\n",
    "\n",
    "os.remove('test/VOCtest_06-Nov-2007.tar')\n",
    "os.remove('train/VOCtrainval_06-Nov-2007.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fd507ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "annot_f = './{}/VOCdevkit/VOC2007/Annotations'\n",
    "image_f = './{}/VOCdevkit/VOC2007/JPEGImages/{}'\n",
    "\n",
    "classes = ['person',\n",
    "          'bird','cat','cow','dog','horse','sheep',\n",
    "          'aeroplane','bicycle','boat','bus','car','motobike','train',\n",
    "          'bottle','chair','dining table','potted plant','sofa','tv/monitor']\n",
    "\n",
    "num_classes = len(classes)\n",
    "feature_size = 7\n",
    "num_bboxes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "483f1304",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from torch.autograd import Variable\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bfb3b859",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hong\\anaconda3\\envs\\thesis\\lib\\site-packages\\torchvision\\io\\image.py:11: UserWarning: Failed to load image Python extension: Could not find module 'C:\\Users\\hong\\anaconda3\\envs\\thesis\\Lib\\site-packages\\torchvision\\image.pyd' (or one of its dependencies). Try using the full path with constructor syntax.\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random, math, time\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import os, xmltodict\n",
    "import os.path as pth\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "## Transformer\n",
    "from random import sample\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29eec423",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_image(image_info, w=448, h=448, transforms=None):\n",
    "    im = np.array(Image.open(image_f.format('train', image_info['image_id'])).convert('RGB').resize((w,h)), dtype=np.uint8)\n",
    "\n",
    "    # Create figure and axes\n",
    "    fig,ax = plt.subplots(1, figsize=(7,7))\n",
    "\n",
    "    bb = image_info['bboxs']\n",
    "    la = image_info['labels']\n",
    "\n",
    "    if transforms:\n",
    "        sample = transforms(image=im, bboxes=bb, category_ids=la)\n",
    "        im = sample['image'].permute(1,2,0).numpy()\n",
    "        bb = sample['bboxes']\n",
    "        la = sample['category_ids']\n",
    "\n",
    "    # Display the image\n",
    "    ax.imshow(im)\n",
    "\n",
    "\n",
    "    # Create a Rectangle patch\n",
    "    for b, l in zip(bb, la):\n",
    "        # top left (x, y) , (w, h)\n",
    "        rect = patches.Rectangle((b[0]*w,b[1]*h),(b[2]-b[0])*w,(b[3]-b[1])*h,linewidth=1,edgecolor='r',facecolor='none')\n",
    "        # Add the patch to the Axes\n",
    "        ax.add_patch(rect)\n",
    "        props = dict(boxstyle='round', facecolor='red', alpha=0.9)\n",
    "        plt.text(b[0]*w, b[1]*h, classes[l], fontsize=10, color='white', bbox=props)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5fc4a925",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3049, 4952)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_infos(annot_f=annot_f, mode='train'):\n",
    "    annot_dir = annot_f.format(mode)\n",
    "    result = []\n",
    "    for ano in [pth.join(annot_dir, ano) for ano in os.listdir(annot_dir)]:\n",
    "        f = open(ano)\n",
    "        info = xmltodict.parse(f.read())['annotation']\n",
    "        image_id = info['filename']\n",
    "        image_size = np.asarray(tuple(map(int, info['size'].values()))[:2], np.int16)\n",
    "        w, h = image_size\n",
    "        box_objects = info['object']\n",
    "        labels = []\n",
    "        bboxs = []\n",
    "        for obj in box_objects:\n",
    "            try:\n",
    "                labels.append(classes.index(obj['name'].lower()))\n",
    "                bboxs.append(tuple(map(int, obj['bndbox'].values())))\n",
    "            except: pass\n",
    "\n",
    "        # Resizing Box, Change x1 y1 x2 y2\n",
    "        # albumentations (normalized box)\n",
    "        bboxs = np.asarray(bboxs, dtype=np.float64)\n",
    "        try:\n",
    "            bboxs[:, [0,2]] /= w\n",
    "            bboxs[:, [1,3]] /= h\n",
    "        except: pass\n",
    "        if bboxs.shape[0] or mode=='test':\n",
    "            result.append({'image_id':image_id, 'image_size':image_size, 'bboxs':bboxs, 'labels':labels})\n",
    "\n",
    "    return result\n",
    "    \n",
    "trval_list = get_infos()\n",
    "test_list = get_infos(mode='test')\n",
    "\n",
    "len(trval_list), len(test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5be98ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tv_idx(tl, k = 0.5):\n",
    "    total_idx = range(tl)\n",
    "    train_idx = sample(total_idx, int(tl*k))\n",
    "    valid_idx = set(total_idx) - set(train_idx)\n",
    "    return train_idx, list(valid_idx)\n",
    "\n",
    "train_idx, valid_idx = get_tv_idx(len(trval_list))\n",
    "\n",
    "trval_list = np.asarray(trval_list)\n",
    "train_list = trval_list[train_idx]\n",
    "valid_list = trval_list[valid_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8a8b1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VOCDataset(Dataset):\n",
    "    def __init__(self, data_list, mode='train', transforms=None):\n",
    "        self.data_list = data_list\n",
    "        self.mode = mode\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        record = self.data_list[idx]\n",
    "        img_id = record['image_id']\n",
    "        bboxs = record['bboxs']\n",
    "        labels = record['labels']\n",
    "\n",
    "        img = Image.open(image_f.format(self.mode, img_id)).convert('RGB') #.resize((800,800))\n",
    "        img = np.array(img)\n",
    "\n",
    "        if self.transforms:\n",
    "            for t in self.transforms:\n",
    "                sample = self.transforms(image=img, bboxes=bboxs, category_ids=labels)\n",
    "                image = sample['image']\n",
    "                bboxs = np.asarray(sample['bboxes'])\n",
    "                labels = np.asarray(sample['category_ids'])\n",
    "\n",
    "\n",
    "        if self.mode=='train':\n",
    "            target = encode(bboxs, labels)\n",
    "            return image, target\n",
    "        else:\n",
    "            return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "45fcb803",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(bboxs, labels):\n",
    "    # Make YoLo Target\n",
    "\n",
    "    S = feature_size\n",
    "    B = num_bboxes\n",
    "    N = 5 * B + num_classes\n",
    "    cell_size = 1.0 / float(S)\n",
    "    # print(bboxs.shape)\n",
    "\n",
    "    box_cxy = (bboxs[:, 2:] + bboxs[:, :2])/2.0\n",
    "    box_wh = bboxs[:, 2:] - bboxs[:, :2]\n",
    "    target = np.zeros((S, S, N))\n",
    "    for b in range(bboxs.shape[0]):\n",
    "        cxy, wh, label = box_cxy[b], box_wh[b], labels[b]\n",
    "        ij = np.ceil(cxy / cell_size) - 1.0\n",
    "        i, j = map(int, ij)\n",
    "        top_left = ij*cell_size\n",
    "        dxy_norm = (cxy-top_left)/cell_size\n",
    "        for k in range(B):\n",
    "            target[j, i, 5*k: 5*(k+1)] = np.r_[dxy_norm, wh, 1]\n",
    "        target[j, i, 5*B+label] = 1.0\n",
    "    return target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e1d46d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_transforms():\n",
    "    return A.Compose([\n",
    "        A.Resize(448,448, always_apply=True, p=1),\n",
    "        # A.Cutout(num_holes=7, max_h_size=16, max_w_size=16, fill_value=0, always_apply=False, p=0.5),\n",
    "        A.RandomBrightnessContrast(p=0.2),\n",
    "        A.HorizontalFlip(),\n",
    "        ToTensor(),\n",
    "    ], bbox_params=A.BboxParams(format='albumentations', label_fields=['category_ids']))\n",
    "\n",
    "def get_test_transforms():\n",
    "    return A.Compose([\n",
    "        A.Resize(448,448, always_apply=True, p=1),\n",
    "        ToTensor(),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3c69d11f",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "`ToTensor` is obsolete and it was removed from Albumentations. Please use `ToTensorV2` instead - https://albumentations.ai/docs/api_reference/pytorch/transforms/#albumentations.pytorch.transforms.ToTensorV2. \n\nIf you need `ToTensor` downgrade Albumentations to version 0.5.2.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17940/857902997.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_ds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVOCDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mget_train_transforms\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mvalid_ds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVOCDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalid_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mget_test_transforms\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtest_ds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVOCDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'test'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mget_test_transforms\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# torch tensor를 batch size만큼 묶어줌\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17940/3744516773.py\u001b[0m in \u001b[0;36mget_train_transforms\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mA\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRandomBrightnessContrast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mA\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mHorizontalFlip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0mToTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     ], bbox_params=A.BboxParams(format='albumentations', label_fields=['category_ids']))\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\thesis\\lib\\site-packages\\albumentations\\pytorch\\transforms.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, num_classes, sigmoid, normalize)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m         raise RuntimeError(\n\u001b[0m\u001b[0;32m     54\u001b[0m             \u001b[1;34m\"`ToTensor` is obsolete and it was removed from Albumentations. Please use `ToTensorV2` instead - \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m             \u001b[1;34m\"https://albumentations.ai/docs/api_reference/pytorch/transforms/\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: `ToTensor` is obsolete and it was removed from Albumentations. Please use `ToTensorV2` instead - https://albumentations.ai/docs/api_reference/pytorch/transforms/#albumentations.pytorch.transforms.ToTensorV2. \n\nIf you need `ToTensor` downgrade Albumentations to version 0.5.2."
     ]
    }
   ],
   "source": [
    "train_ds = VOCDataset(train_list, transforms=get_train_transforms())\n",
    "valid_ds = VOCDataset(valid_list, transforms=get_test_transforms())\n",
    "test_ds = VOCDataset(test_list, mode='test', transforms=get_test_transforms())\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images, targets = zip(*batch)\n",
    "    return torch.cat([img.reshape(-1, 3, 448, 448) for img in images], 0), torch.FloatTensor(targets)\n",
    "\n",
    "def test_collate_fn(batch):\n",
    "    images = batch\n",
    "    return torch.cat([img.reshape(-1, 3, 448, 448) for img in images], 0)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "valid_loader = DataLoader(valid_ds, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_ds, batch_size=1, shuffle=False, collate_fn=test_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683c7fa1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
